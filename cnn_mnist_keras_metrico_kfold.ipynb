{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405b0d67",
   "metadata": {},
   "source": [
    "# CNN em Keras para MNIST (Reprodução do Artigo)\n",
    "\n",
    "Este notebook implementa, em **TensorFlow/Keras**, a CNN descrita no artigo\n",
    "\n",
    "> *AN OPTIMIZED CONVOLUTIONAL NEURAL NETWORK FOR HANDWRITTEN DIGITAL RECOGNITION CLASSIFICATION*\n",
    "\n",
    "Aqui fazemos:\n",
    "\n",
    "1. Preparação do dataset MNIST  \n",
    "2. Definição de um modelo CNN com profundidade variável (1, 2 ou 3 blocos Conv+Pool)  \n",
    "3. Treinamento com **5-fold cross-validation**  \n",
    "4. Experimentos variando:\n",
    "   - **taxa de aprendizagem (learning rate)**  \n",
    "   - **número de épocas (epochs)**  \n",
    "   - **profundidade da rede (número de blocos Conv+Pool)**  \n",
    "5. Geração de **gráficos** para análise visual dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports principais\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f222e",
   "metadata": {},
   "source": [
    "## 1. Carregando e preparando o dataset MNIST\n",
    "\n",
    "- Imagens 28x28 em escala de cinza (0–255)  \n",
    "- Normalizamos para o intervalo **[0, 1]**  \n",
    "- Adicionamos um eixo de canal para ficar no formato `(28, 28, 1)`  \n",
    "- Codificamos os rótulos em *one-hot* (10 classes: dígitos de 0 a 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c952902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalização para [0,1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Adicionar eixo de canal (grayscale)\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# One-hot encoding dos rótulos\n",
    "num_classes = 10\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train.shape, y_train_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778858fc",
   "metadata": {},
   "source": [
    "## 2. Definição do modelo CNN (com profundidade variável)\n",
    "\n",
    "O artigo trabalha com a ideia de **profundidade** da rede como o número de blocos:\n",
    "\n",
    "> bloco = `Conv2D → ReLU → MaxPooling2D`\n",
    "\n",
    "Assim, temos:\n",
    "\n",
    "- **1 bloco** → rede mais rasa  \n",
    "- **2 blocos** → rede moderadamente profunda (melhor desempenho no artigo)  \n",
    "- **3 blocos** → rede mais profunda (piora um pouco o desempenho)  \n",
    "\n",
    "Após os blocos, usamos:\n",
    "\n",
    "- `Flatten` para achatar o mapa de características em um vetor  \n",
    "- `Dense(128, ReLU)` para interpretação das features  \n",
    "- `Dense(10, Softmax)` para classificação em 10 dígitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(num_blocks=1, learning_rate=0.01, momentum=0.9):\n",
    "    \"\"\"Cria a CNN conforme a descrição do artigo, com num_blocks blocos Conv+Pool.\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Primeiro bloco Conv+Pool (com shape de entrada definido)\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(28, 28, 1),\n",
    "        )\n",
    "    )\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Blocos adicionais (para profundidade 2 ou 3)\n",
    "    for _ in range(num_blocks - 1):\n",
    "        model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Classificador\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\"))  # tamanho não especificado no artigo\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    # Otimizador SGD com momentum (como no artigo)\n",
    "    optimizer = SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Exemplo: modelo raso com 1 bloco\n",
    "model_example = create_cnn_model(num_blocks=1)\n",
    "model_example.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b328fbdd",
   "metadata": {},
   "source": [
    "## 3. Funções de treino e avaliação com 5-fold cross-validation\n",
    "\n",
    "O artigo utiliza **5-fold cross-validation** para:\n",
    "\n",
    "- Reduzir variância na avaliação  \n",
    "- Obter **média** e **desvio padrão** da acurácia  \n",
    "\n",
    "Vamos implementar uma função que:\n",
    "\n",
    "- Recebe `num_blocks`, `learning_rate`, `epochs`  \n",
    "- Faz o *split* em 5 folds  \n",
    "- Treina o modelo em cada fold  \n",
    "- Retorna acurácia média e desvio padrão em %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(num_blocks, learning_rate, epochs, batch_size=32, verbose=0, verbose_metrics=True):\n",
    "    \"\"\"Executa treinamento com 5-fold cross-validation e calcula métricas detalhadas.\n",
    "\n",
    "    Retorna um dicionário com:\n",
    "        - acc_mean, acc_std\n",
    "        - f1_mean, f1_std\n",
    "        - prec_mean, prec_std\n",
    "        - rec_mean, rec_std\n",
    "        - train_time_sec\n",
    "        - confusion_matrix (agregada em todas as folds)\n",
    "        - classification_report (string com precisão/recall/F1 por classe)\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(x_train), start=1):\n",
    "        x_tr, x_val = x_train[train_idx], x_train[val_idx]\n",
    "        y_tr, y_val = y_train_cat[train_idx], y_train_cat[val_idx]\n",
    "\n",
    "        model = create_cnn_model(\n",
    "            num_blocks=num_blocks,\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=0.9,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            x_tr,\n",
    "            y_tr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # Predições no conjunto de validação desta fold\n",
    "        y_val_prob = model.predict(x_val, verbose=0)\n",
    "        y_val_pred = np.argmax(y_val_prob, axis=1)\n",
    "        y_val_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "        all_y_true.extend(y_val_true.tolist())\n",
    "        all_y_pred.extend(y_val_pred.tolist())\n",
    "\n",
    "        # Métricas por fold\n",
    "        acc = accuracy_score(y_val_true, y_val_pred) * 100.0\n",
    "        f1_macro = f1_score(y_val_true, y_val_pred, average=\"macro\") * 100.0\n",
    "        prec_macro = precision_score(\n",
    "            y_val_true,\n",
    "            y_val_pred,\n",
    "            average=\"macro\",\n",
    "            zero_division=0,\n",
    "        ) * 100.0\n",
    "        rec_macro = recall_score(\n",
    "            y_val_true,\n",
    "            y_val_pred,\n",
    "            average=\"macro\",\n",
    "            zero_division=0,\n",
    "        ) * 100.0\n",
    "\n",
    "        accs.append(acc)\n",
    "        f1s.append(f1_macro)\n",
    "        precs.append(prec_macro)\n",
    "        recs.append(rec_macro)\n",
    "\n",
    "        if verbose_metrics:\n",
    "            print(\n",
    "                f\"Fold {fold_idx}: Acc={acc:.3f}%, F1_macro={f1_macro:.3f}%, \"\n",
    "                f\"Precision_macro={prec_macro:.3f}%, Recall_macro={rec_macro:.3f}%\"\n",
    "            )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    all_y_true = np.array(all_y_true)\n",
    "    all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "    # Métricas agregadas em todas as folds\n",
    "    cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "    report = classification_report(all_y_true, all_y_pred, digits=4)\n",
    "\n",
    "    summary = {\n",
    "        \"acc_mean\": float(np.mean(accs)),\n",
    "        \"acc_std\": float(np.std(accs)),\n",
    "        \"f1_mean\": float(np.mean(f1s)),\n",
    "        \"f1_std\": float(np.std(f1s)),\n",
    "        \"prec_mean\": float(np.mean(precs)),\n",
    "        \"prec_std\": float(np.std(precs)),\n",
    "        \"rec_mean\": float(np.mean(recs)),\n",
    "        \"rec_std\": float(np.std(recs)),\n",
    "        \"train_time_sec\": float(total_time),\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": report,\n",
    "    }\n",
    "\n",
    "    if verbose_metrics:\n",
    "        print(f\"\\nTempo total de treinamento (5 folds): {total_time:.2f} segundos\")\n",
    "        print(\"\\nRelatório de classificação agregado (todas as folds):\")\n",
    "        print(report)\n",
    "        print(\"Matriz de confusão agregada (todas as folds):\")\n",
    "        print(cm)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d0bda",
   "metadata": {},
   "source": [
    "## 4. Experimento 1 – Variando a profundidade da rede\n",
    "\n",
    "Aqui reproduzimos o experimento de **profundidade (model depth)**:\n",
    "\n",
    "- Testamos **1, 2 e 3 blocos Conv+Pool**  \n",
    "- Mantemos:\n",
    "  - `learning_rate = 0.01`  \n",
    "  - `epochs = 10` (baseline)  \n",
    "\n",
    "Em seguida, ploteremos um gráfico de barras com a acurácia média de cada configuração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda52f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1, 2, 3]\n",
    "\n",
    "# Listas para guardar as métricas de acurácia (mantendo compatível com os gráficos)\n",
    "depth_means = []\n",
    "depth_stds = []\n",
    "\n",
    "# Listas extras para outras métricas\n",
    "depth_f1_means = []\n",
    "depth_f1_stds = []\n",
    "depth_prec_means = []\n",
    "depth_prec_stds = []\n",
    "depth_rec_means = []\n",
    "depth_rec_stds = []\n",
    "\n",
    "depth_results = []  # guarda o dicionário completo de cada experimento\n",
    "\n",
    "for d in depths:\n",
    "    print(f\"\\n=== Profundidade: {d} bloco(s) Conv+Pool ===\")\n",
    "    summary = run_kfold(\n",
    "        num_blocks=d,\n",
    "        learning_rate=0.01,\n",
    "        epochs=10,\n",
    "        verbose=0,\n",
    "        verbose_metrics=True,\n",
    "    )\n",
    "\n",
    "    depth_results.append(summary)\n",
    "\n",
    "    # Acurácia (para os gráficos já existentes)\n",
    "    depth_means.append(summary[\"acc_mean\"])\n",
    "    depth_stds.append(summary[\"acc_std\"])\n",
    "\n",
    "    # Outras métricas\n",
    "    depth_f1_means.append(summary[\"f1_mean\"])\n",
    "    depth_f1_stds.append(summary[\"f1_std\"])\n",
    "    depth_prec_means.append(summary[\"prec_mean\"])\n",
    "    depth_prec_stds.append(summary[\"prec_std\"])\n",
    "    depth_rec_means.append(summary[\"rec_mean\"])\n",
    "    depth_rec_stds.append(summary[\"rec_std\"])\n",
    "\n",
    "    print(\n",
    "        \"Resumo final (5-fold): \"\n",
    "        f\"Acurácia = {summary['acc_mean']:.3f}% (±{summary['acc_std']:.3f}%), \"\n",
    "        f\"F1-macro = {summary['f1_mean']:.3f}% (±{summary['f1_std']:.3f}%), \"\n",
    "        f\"Precision-macro = {summary['prec_mean']:.3f}% (±{summary['prec_std']:.3f}%), \"\n",
    "        f\"Recall-macro = {summary['rec_mean']:.3f}% (±{summary['rec_std']:.3f}%), \"\n",
    "        f\"Tempo treino = {summary['train_time_sec']:.2f} s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed276395",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "x_pos = np.arange(len(depths))\n",
    "\n",
    "plt.bar(x_pos, depth_means, yerr=depth_stds, capsize=6)\n",
    "\n",
    "plt.xticks(x_pos, [str(d) for d in depths])\n",
    "plt.xlabel(\"Número de blocos Conv+Pool (profundidade)\")\n",
    "plt.ylabel(\"Acurácia média (%)\")\n",
    "plt.title(\"Acurácia vs Profundidade da CNN (5-fold)\")\n",
    "\n",
    "# Zoom automático usando zip (funciona com listas)\n",
    "min_val = min(m - s for m, s in zip(depth_means, depth_stds))\n",
    "max_val = max(m + s for m, s in zip(depth_means, depth_stds))\n",
    "\n",
    "plt.ylim(min_val - 0.3, max_val + 0.3)\n",
    "\n",
    "# Exibir valores no topo das barras\n",
    "for i, v in enumerate(depth_means):\n",
    "    plt.text(i, v + 0.05, f\"{v:.2f}%\", ha='left', fontsize=10)\n",
    "    \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab405f02",
   "metadata": {},
   "source": [
    "## 5. Experimento 2 – Variando o *learning rate*\n",
    "\n",
    "Agora avaliamos o impacto do **learning rate**:\n",
    "\n",
    "- Mantemos:\n",
    "  - 1 bloco Conv+Pool (modelo baseline)  \n",
    "  - 10 épocas  \n",
    "- Testamos os valores usados no artigo:\n",
    "  - 0.01, 0.05, 0.08, 0.10, 0.15  \n",
    "\n",
    "Em seguida, ploteremos um gráfico com a acurácia média por valor de learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [0.01, 0.05, 0.08, 0.1, 0.15]\n",
    "\n",
    "# Listas para acurácia (mantendo compatível com o gráfico existente)\n",
    "lr_means = []\n",
    "lr_stds = []\n",
    "\n",
    "# Listas extras para outras métricas\n",
    "lr_f1_means = []\n",
    "lr_f1_stds = []\n",
    "lr_prec_means = []\n",
    "lr_prec_stds = []\n",
    "lr_rec_means = []\n",
    "lr_rec_stds = []\n",
    "\n",
    "lr_results = []\n",
    "\n",
    "for lr in lrs:\n",
    "    print(f\"\\n=== Experimento: learning rate = {lr} ===\")\n",
    "    summary = run_kfold(\n",
    "        num_blocks=1,\n",
    "        learning_rate=lr,\n",
    "        epochs=10,\n",
    "        verbose=0,\n",
    "        verbose_metrics=True,\n",
    "    )\n",
    "\n",
    "    lr_results.append(summary)\n",
    "\n",
    "    lr_means.append(summary[\"acc_mean\"])\n",
    "    lr_stds.append(summary[\"acc_std\"])\n",
    "\n",
    "    lr_f1_means.append(summary[\"f1_mean\"])\n",
    "    lr_f1_stds.append(summary[\"f1_std\"])\n",
    "    lr_prec_means.append(summary[\"prec_mean\"])\n",
    "    lr_prec_stds.append(summary[\"prec_std\"])\n",
    "    lr_rec_means.append(summary[\"rec_mean\"])\n",
    "    lr_rec_stds.append(summary[\"rec_std\"])\n",
    "\n",
    "    print(\n",
    "        \"Resumo final (5-fold): \"\n",
    "        f\"Acurácia = {summary['acc_mean']:.3f}% (±{summary['acc_std']:.3f}%), \"\n",
    "        f\"F1-macro = {summary['f1_mean']:.3f}% (±{summary['f1_std']:.3f}%), \"\n",
    "        f\"Precision-macro = {summary['prec_mean']:.3f}% (±{summary['prec_std']:.3f}%), \"\n",
    "        f\"Recall-macro = {summary['rec_mean']:.3f}% (±{summary['rec_std']:.3f}%), \"\n",
    "        f\"Tempo treino = {summary['train_time_sec']:.2f} s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico: acurácia média x learning rate\n",
    "plt.figure()\n",
    "x_pos = np.arange(len(lrs))\n",
    "plt.plot(x_pos, lr_means, marker=\"o\")\n",
    "plt.xticks(x_pos, [str(lr) for lr in lrs])\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Acurácia média (%)\")\n",
    "plt.title(\"Acurácia vs Learning rate (5-fold)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703c41c",
   "metadata": {},
   "source": [
    "## 6. Experimento 3 – Variando o número de épocas\n",
    "\n",
    "Agora estudamos o efeito de **mais épocas de treino** na performance:\n",
    "\n",
    "- Mantemos:\n",
    "  - 1 bloco Conv+Pool  \n",
    "  - learning rate = 0.01  \n",
    "- Testamos:\n",
    "  - 10, 15, 20, 25, 30 épocas  \n",
    "\n",
    "Em seguida, veremos o gráfico de acurácia média por número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d49259",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [10, 15, 20, 25, 30]\n",
    "\n",
    "# Listas de acurácia (compatíveis com o gráfico existente)\n",
    "ep_means = []\n",
    "ep_stds = []\n",
    "\n",
    "# Listas extras para outras métricas\n",
    "ep_f1_means = []\n",
    "ep_f1_stds = []\n",
    "ep_prec_means = []\n",
    "ep_prec_stds = []\n",
    "ep_rec_means = []\n",
    "ep_rec_stds = []\n",
    "\n",
    "ep_results = []\n",
    "\n",
    "for ep in epochs_list:\n",
    "    print(f\"\\n=== Experimento: épocas = {ep} ===\")\n",
    "    summary = run_kfold(\n",
    "        num_blocks=1,\n",
    "        learning_rate=0.01,\n",
    "        epochs=ep,\n",
    "        verbose=0,\n",
    "        verbose_metrics=True,\n",
    "    )\n",
    "\n",
    "    ep_results.append(summary)\n",
    "\n",
    "    ep_means.append(summary[\"acc_mean\"])\n",
    "    ep_stds.append(summary[\"acc_std\"])\n",
    "\n",
    "    ep_f1_means.append(summary[\"f1_mean\"])\n",
    "    ep_f1_stds.append(summary[\"f1_std\"])\n",
    "    ep_prec_means.append(summary[\"prec_mean\"])\n",
    "    ep_prec_stds.append(summary[\"prec_std\"])\n",
    "    ep_rec_means.append(summary[\"rec_mean\"])\n",
    "    ep_rec_stds.append(summary[\"rec_std\"])\n",
    "\n",
    "    print(\n",
    "        \"Resumo final (5-fold): \"\n",
    "        f\"Acurácia = {summary['acc_mean']:.3f}% (±{summary['acc_std']:.3f}%), \"\n",
    "        f\"F1-macro = {summary['f1_mean']:.3f}% (±{summary['f1_std']:.3f}%), \"\n",
    "        f\"Precision-macro = {summary['prec_mean']:.3f}% (±{summary['prec_std']:.3f}%), \"\n",
    "        f\"Recall-macro = {summary['rec_mean']:.3f}% (±{summary['rec_std']:.3f}%), \"\n",
    "        f\"Tempo treino = {summary['train_time_sec']:.2f} s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903350a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico: acurácia média x número de épocas\n",
    "plt.figure()\n",
    "x_pos = np.arange(len(epochs_list))\n",
    "plt.plot(x_pos, ep_means, marker=\"o\")\n",
    "plt.xticks(x_pos, [str(ep) for ep in epochs_list])\n",
    "plt.xlabel(\"Número de épocas\")\n",
    "plt.ylabel(\"Acurácia média (%)\")\n",
    "plt.title(\"Acurácia vs Número de épocas (5-fold)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fafcf",
   "metadata": {},
   "source": [
    "## 7. Conclusões\n",
    "\n",
    "A partir dos experimentos, pode-se comparar com os resultados do artigo:\n",
    "\n",
    "- **Profundidade**: geralmente, 2 blocos Conv+Pool tendem a performar melhor que 1 ou 3 blocos.  \n",
    "- **Learning rate**: valores muito altos (ex: 0.1, 0.15) degradam a acurácia; algo próximo a **0.01** costuma ser mais estável.  \n",
    "- **Número de épocas**: aumentar as épocas nem sempre melhora indefinidamente; há um ponto de retorno decrescente, e o custo computacional cresce.  \n",
    "\n",
    "É possivel ajustar os hiperparâmetros e repetir os blocos de código dos experimentos para novas análises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
